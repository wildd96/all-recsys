{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "a6164808-2b20-4d30-8f2a-3922335e7519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This is a starting point\n",
    "#uncomment below to install merlin library\n",
    "#!pip install merlin, nvtabular, merlin.models\n",
    "#I had to downgrade keras to 12.2.0 there maybe issues regarding tensorflow in the future\n",
    "#run lines below if you have also have an error ab not finding a keras package\n",
    "\n",
    "#!pip uninstall keras\n",
    "#!pip install keras==2.12.0\n",
    "\n",
    "\n",
    "#These are the same imports from\n",
    "#https://github.com/NVIDIA-Merlin/models/blob/main/examples/02-Merlin-Models-and-NVTabular-integration.ipynb\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import nvtabular as nvt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from merlin.models.utils.example_utils import workflow_fit_transform\n",
    "import merlin.io\n",
    "import tensorflow as tf\n",
    "\n",
    "import merlin.models.tf as mm\n",
    "from merlin.io.dataset import Dataset\n",
    "from nvtabular.ops import *\n",
    "from merlin.core.utils import download_file\n",
    "from merlin.schema.tags import Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a340cdef-e86d-43fe-ac78-a6ff62a25aed",
   "metadata": {},
   "source": [
    "# Load and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "95493f8f-f037-4bc2-b737-3e3cb0c14967",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/andrew/Desktop/projects/recsys_data/2023-10-05 9_23pm.csv').dropna()\n",
    "df = df.drop(columns = ['CLICKSTREAM_EVENTS_TOTAL.1'])\n",
    "#  ^ This needs point to your local .csv too big for github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "032fb1a0-26b4-42c1-a348-b103f9d80e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accidentally added an extra CLICKSTREAM_EVENTS_TOTAL column\n",
    "df['TARGET'] = 0\n",
    "df.loc[df['EVENT_NAME'] == 'order', 'TARGET'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "96820674-e0a6-457d-a913-483194735589",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03777583047877001"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['TARGET'].sum()/len(df['TARGET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "6dd79ec9-f741-4e0d-a46e-123e36c4f68c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/anaconda3/lib/python3.11/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/Users/andrew/anaconda3/lib/python3.11/site-packages/merlin/io/parquet.py:343: UserWarning: Row group memory size (1394257212) (bytes) of parquet file is bigger than requested part_size (1073741824) for the NVTabular dataset.A row group memory size of 128 MB is generally recommended. You can find info on how to set the row group size of parquet files in https://nvidia-merlin.github.io/NVTabular/main/resources/troubleshooting.html#setting-the-row-group-size-for-the-parquet-files\n",
      "  warnings.warn(\n",
      "/Users/andrew/anaconda3/lib/python3.11/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#num rows\n",
    "data_size = df.shape[0]\n",
    "\n",
    "#We're doing a 20/80 train/validate split change the .2 to change the split\n",
    "train_split_ratio = int(.2 * data_size)\n",
    "\n",
    "train_data = df[train_split_ratio:].to_parquet(\"train_data.parquet\")\n",
    "valid_data = df[:train_split_ratio].to_parquet(\"valid_data.parquet\")\n",
    "\n",
    "train = Dataset(\"train_data.parquet\")\n",
    "valid = Dataset(\"valid_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b424279-225c-4eef-b583-0427bc954c57",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770badc5-5a3d-4282-a455-fef1eff25209",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fc862f-90f9-4db2-858b-bc4e93925ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da5efa1f-91e7-4c32-a882-6bbe6fe26555",
   "metadata": {},
   "source": [
    "# NVTabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "5f247a55-cbdd-4cce-8b47-7e939e0984e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is how to iniate a dataset using NVtabular an NVIDIA \n",
    "#optimized ETL/feature engineering(i think?) library\n",
    "#data = nvt.io.dataset.Dataset('data/reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "c65d68b7-27c7-436e-9d49-58c91cc067fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "7d3c6c8f-282a-489f-8e5c-51411edc1435",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categorical_user_features = ['COUNTRY', 'DERIVED_GENDER_BY_NAME', 'EVENT_NAME']\n",
    "categorical_item_features = ['STYLE', 'TAXONOMY_STYLE', 'COLOR_NAME', 'PRODUCT_CLASS', 'PRODUCT_SUBCLASS', 'TEAM', 'FRANCHISE', 'PRODUCT_GROUP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "f941f6d0-41be-4177-aac5-a884aa7bb630",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Features needed to Transform still: FIRST_PURCHASE_AT, FIRST_VISIT_AT, LATEST_VISIT_AT\n",
    "# LATEST_PURCHASE_AT, EVENT_TIMESTAMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "c397eb73-c1bc-4e1b-838c-d72a85c2adb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DON'T FORGET TARGET ENCODING CATEGORICAL VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "74895def-f932-4e9c-b1f8-0d78da2b7571",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_id = [\"USER_ID\"] >> Categorify(dtype = \"int32\") >> TagAsUserID()\n",
    "item_id = [\"ITEM_ID\"] >> Categorify(dtype = \"int32\") >> TagAsItemID()\n",
    "item_features = categorical_item_features >> Categorify(dtype = \"int32\") >> TagAsItemFeatures()\n",
    "user_features = categorical_user_features >> Categorify(dtype = \"int32\") >> TagAsUserFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "9d3d475a-df44-4291-bbd7-1f534e8d5c34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Make this more succinct\n",
    "CLICKSTREAM_EVENTS_TOTAL = (\n",
    "    ['CLICKSTREAM_EVENTS_TOTAL']\n",
    "    >>FillMissing(0)\n",
    "    >>LogOp()\n",
    "    >>Normalize()\n",
    "    >>LambdaOp(lambda col: col.astype(\"float32\"))\n",
    "    >>TagAsUserFeatures()\n",
    ")\n",
    "\n",
    "#has the price changed over time?\n",
    "PRICE_INFORMATION = (\n",
    "    ['PRICE_INFORMATION']\n",
    "    >>FillMissing(0)\n",
    "    >>LogOp()\n",
    "    >>Normalize()\n",
    "    >>LambdaOp(lambda col: col.astype(\"float32\"))\n",
    "    >>TagAsItemFeatures()\n",
    ")\n",
    "\n",
    "AVG_REVIEW_SCORE = (\n",
    "    ['AVG_REVIEW_SCORE']\n",
    "    >>FillMissing(0)\n",
    "    >>LogOp()\n",
    "    >>Normalize()\n",
    "    >>LambdaOp(lambda col: col.astype(\"float32\"))\n",
    "    >>TagAsItemFeatures()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "690179d7-afc5-4e32-998b-0ce599c38c9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categorical_features = (categorical_item_features) #Just a place holder... redo with better categories\n",
    "label = nvt.ColumnSelector(['TARGET'])\n",
    "te_features = categorical_features >> TargetEncoding(label)\n",
    "te_features_norm = te_features >> Normalize() >> LambdaOp(lambda col: col.astype('float32')) >> TagAsItemFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "61b9e12c-51c9-454e-b0e9-94d27449f741",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target = (\n",
    "    ['TARGET'] >> AddMetadata(tags=[Tags.BINARY_CLASSIFICATION, \"TARGET\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "19e65950-c396-4a0c-be31-65adaaa63418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = (\n",
    "    user_id + \n",
    "    target + \n",
    "    CLICKSTREAM_EVENTS_TOTAL + \n",
    "    item_id +\n",
    "    te_features_norm +\n",
    "    PRICE_INFORMATION +\n",
    "    AVG_REVIEW_SCORE\n",
    ")\n",
    "\n",
    "workflow = nvt.Workflow(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "5261238e-74f6-42ab-b059-62ff5cd24f25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/anaconda3/lib/python3.11/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/Users/andrew/anaconda3/lib/python3.11/site-packages/merlin/io/parquet.py:343: UserWarning: Row group memory size (1394257212) (bytes) of parquet file is bigger than requested part_size (1073741824) for the NVTabular dataset.A row group memory size of 128 MB is generally recommended. You can find info on how to set the row group size of parquet files in https://nvidia-merlin.github.io/NVTabular/main/resources/troubleshooting.html#setting-the-row-group-size-for-the-parquet-files\n",
      "  warnings.warn(\n",
      "/Users/andrew/anaconda3/lib/python3.11/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/Users/andrew/anaconda3/lib/python3.11/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/andrew/anaconda3/lib/python3.11/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/Users/andrew/anaconda3/lib/python3.11/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/andrew/anaconda3/lib/python3.11/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/Users/andrew/anaconda3/lib/python3.11/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/andrew/anaconda3/lib/python3.11/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18 s, sys: 4.07 s, total: 22 s\n",
      "Wall time: 22.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/anaconda3/lib/python3.11/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_data = Dataset('train_data.parquet') #Direct to your local files\n",
    "valid_data = Dataset('valid_data.parquet')\n",
    "\n",
    "workflow.fit(train_data)\n",
    "\n",
    "workflow.transform(train_data).to_parquet(\n",
    "    'train_data_transformed' #Feel free to replace with your own local file path\n",
    ")\n",
    "workflow.transform(valid_data).to_parquet(\n",
    "    'valid_data_transformed' #same here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bcd359-0fbc-4ac2-b660-23d57e375134",
   "metadata": {},
   "source": [
    "## Transform Timestamped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "e3470a1c-4786-4ccd-a592-395036db4286",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/anaconda3/lib/python3.11/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/Users/andrew/anaconda3/lib/python3.11/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train = Dataset('train_data_transformed', engine = 'parquet')\n",
    "valid = Dataset('valid_data_transformed', engine = 'parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "7c6271c3-6eff-47c2-9759-120c352e6cb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "schema = train.schema\n",
    "target_column = schema.select_by_tag(Tags.TARGET).column_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "c789862a-c9a4-48b8-ae0c-06b26ec2c75b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Dataset' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[429], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTARGET\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Dataset' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "train['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "056922d0-f08a-43d7-9bf7-182c20d57086",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Loader.__init__() got an unexpected keyword argument 'learning_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[413], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m mm\u001b[38;5;241m.\u001b[39mDLRMModel(\n\u001b[1;32m      2\u001b[0m     schema,\n\u001b[1;32m      3\u001b[0m     embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     prediction_tasks\u001b[38;5;241m=\u001b[39mmm\u001b[38;5;241m.\u001b[39mBinaryOutput(target_column)\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madagrad\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mAUC()])\n\u001b[0;32m---> 10\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train, validation_data\u001b[38;5;241m=\u001b[39mvalid, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/merlin/models/tf/models/base.py:1382\u001b[0m, in \u001b[0;36mBaseModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, train_metrics_steps, pre, **kwargs)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[1;32m   1358\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1359\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1380\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1381\u001b[0m ):\n\u001b[0;32m-> 1382\u001b[0m     x \u001b[38;5;241m=\u001b[39m _maybe_convert_merlin_dataset(x, batch_size, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1383\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_set_schema(x)\n\u001b[1;32m   1385\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/merlin/models/tf/models/base.py:2684\u001b[0m, in \u001b[0;36m_maybe_convert_merlin_dataset\u001b[0;34m(data, batch_size, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m   2681\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batch_size:\n\u001b[1;32m   2682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size must be specified when using merlin-dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2684\u001b[0m data \u001b[38;5;241m=\u001b[39m Loader(data, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39mshuffle, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2686\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m shuffle:\n\u001b[1;32m   2687\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshuffle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/merlin/models/tf/loader.py:314\u001b[0m, in \u001b[0;36mLoader.__init__\u001b[0;34m(self, paths_or_dataset, batch_size, label_names, feature_columns, cat_names, cont_names, engine, shuffle, seed_fn, buffer_size, device, parts_per_chunk, reader_kwargs, global_size, global_rank, drop_last, sparse_names, sparse_max, sparse_as_dense, schema, **loader_kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m     global_rank \u001b[38;5;241m=\u001b[39m global_rank \u001b[38;5;129;01mor\u001b[39;00m hvd\u001b[38;5;241m.\u001b[39mrank()\n\u001b[1;32m    312\u001b[0m     seed_fn \u001b[38;5;241m=\u001b[39m seed_fn \u001b[38;5;129;01mor\u001b[39;00m get_default_hvd_seed_fn()\n\u001b[0;32m--> 314\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    315\u001b[0m     dataset,\n\u001b[1;32m    316\u001b[0m     batch_size,\n\u001b[1;32m    317\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39mshuffle,\n\u001b[1;32m    318\u001b[0m     seed_fn\u001b[38;5;241m=\u001b[39mseed_fn,\n\u001b[1;32m    319\u001b[0m     parts_per_chunk\u001b[38;5;241m=\u001b[39mparts_per_chunk,\n\u001b[1;32m    320\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m    321\u001b[0m     global_size\u001b[38;5;241m=\u001b[39mglobal_size,\n\u001b[1;32m    322\u001b[0m     global_rank\u001b[38;5;241m=\u001b[39mglobal_rank,\n\u001b[1;32m    323\u001b[0m     drop_last\u001b[38;5;241m=\u001b[39mdrop_last,\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloader_kwargs,\n\u001b[1;32m    325\u001b[0m )\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# Override these parameters after initializing the parent dataloader\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# class since the new dataloader will use sparse tensors for list\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# columns by default, but sparse tensors were disabled by default\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# and were optional in the old version of merlin.loader.\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse_names \u001b[38;5;241m=\u001b[39m sparse_names \u001b[38;5;129;01mor\u001b[39;00m []\n",
      "\u001b[0;31mTypeError\u001b[0m: Loader.__init__() got an unexpected keyword argument 'learning_rate'"
     ]
    }
   ],
   "source": [
    "model = mm.DLRMModel(\n",
    "    schema,\n",
    "    embedding_dim=64,\n",
    "    bottom_block=mm.MLPBlock([128, 64]),\n",
    "    top_block=mm.MLPBlock([128, 64, 32]),\n",
    "    prediction_tasks=mm.BinaryOutput(target_column)\n",
    ")\n",
    "\n",
    "model.compile(optimizer=\"adagrad\", metrics = [tf.keras.metrics.AUC()])\n",
    "model.fit(train, validation_data=valid, batch_size=1024, epochs = 1, learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "b261d9e5-55bf-43ac-bdd1-ee9c1d9d5283",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298/298 [==============================] - 1s 3ms/step - loss: nan - auc_1: 0.5000 - regularization_loss: 0.0000e+00 - loss_batch: nan\n"
     ]
    }
   ],
   "source": [
    "metrics = model.evaluate(valid, batch_size = 1024, return_dict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67e832e-3893-46cf-86b3-312b2ccbf4f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5a4afd-1d11-4390-809f-107476ec31e7",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a91203f-c19f-4e46-addf-33ea46aa544e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d329a2-d750-4f71-8f28-78d33414a716",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = mm.DLRMModel(\n",
    "    train_transformed.schema,\n",
    "    embedding_dim=64,\n",
    "    bottom_block=mm.MLPBlock([128, 64]),\n",
    "    top_block=mm.MLPBlock([128, 64, 32]),\n",
    "    prediction_tasks=mm.BinaryClassificationTask(train_transformed.schema)\n",
    ")\n",
    "\n",
    "model.compile(optimizer=\"adagrad\", run_eagerly=False)\n",
    "model.fit(train_transformed, validation_data=valid_transformed, batch_size=1024)\n",
    "eval_metrics = model.evaluate(valid_transformed, batch_size=1024, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cedc2fe-7756-4f09-8e26-4d6f2a9278ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79976dd-8cb2-4a09-adcc-33f36e19a9d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
